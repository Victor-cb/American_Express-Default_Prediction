{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']\n",
    "\n",
    "train = pd.read_parquet(\"../data/processed/train_withfeatures.parquet\")\n",
    "test = pd.read_parquet(\"../data/raw/test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VERSION NAME FOR SAVED MODEL FILES\n",
    "VER = '01'\n",
    "\n",
    "# TRAIN RANDOM SEED\n",
    "SEED = 42\n",
    "\n",
    "# FILL NAN VALUE\n",
    "NAN_VALUE = -127 # will fit in int8\n",
    "\n",
    "# FOLDS PER MODEL\n",
    "FOLDS = 5\n",
    "\n",
    "# TRAIN FOLD\n",
    "TRAIN_PATH = \"../data/processed/train.parquet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Competition metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def amex_metric(y_true, y_pred):\n",
    "\n",
    "    labels     = np.transpose(np.array([y_true, y_pred]))\n",
    "    labels     = labels[labels[:, 1].argsort()[::-1]]\n",
    "    weights    = np.where(labels[:,0]==0, 20, 1)\n",
    "    cut_vals   = labels[np.cumsum(weights) <= int(0.04 * np.sum(weights))]\n",
    "    top_four   = np.sum(cut_vals[:,0]) / np.sum(labels[:,0])\n",
    "\n",
    "    gini = [0,0]\n",
    "    for i in [1,0]:\n",
    "        labels         = np.transpose(np.array([y_true, y_pred]))\n",
    "        labels         = labels[labels[:, i].argsort()[::-1]]\n",
    "        weight         = np.where(labels[:,0]==0, 20, 1)\n",
    "        weight_random  = np.cumsum(weight / np.sum(weight))\n",
    "        total_pos      = np.sum(labels[:, 0] *  weight)\n",
    "        cum_pos_found  = np.cumsum(labels[:, 0] * weight)\n",
    "        lorentz        = cum_pos_found / total_pos\n",
    "        gini[i]        = np.sum((lorentz - weight_random) * weight)\n",
    "\n",
    "    return 0.5 * (gini[1]/gini[0] + top_four)\n",
    "def lgb_amex_metric(y_pred, y_true):\n",
    "    y_true = y_true.get_label()\n",
    "    return 'amex_metric', amex_metric(y_true, y_pred), True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 918 features!\n"
     ]
    }
   ],
   "source": [
    "# FEATURES\n",
    "FEATURES = train.columns[1:-1]\n",
    "print(f'There are {len(FEATURES)} features!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = {\n",
    "        'num_leaves': 10,\n",
    "        'max_bin': 127,\n",
    "        'min_data_in_leaf': 11,\n",
    "        'learning_rate': 0.02,\n",
    "        'min_sum_hessian_in_leaf': 0.00245,\n",
    "        'bagging_fraction': 1.0, \n",
    "        'bagging_freq': 5, \n",
    "        'feature_fraction': 0.05,\n",
    "        'lambda_l1': 4.972,\n",
    "        'lambda_l2': 2.276,\n",
    "        'min_gain_to_split': 0.65,\n",
    "        'max_depth': 14,\n",
    "        'save_binary': True,\n",
    "        'seed': 1337,\n",
    "        'feature_fraction_seed': 1337,\n",
    "        'bagging_seed': 1337,\n",
    "        'drop_seed': 1337,\n",
    "        'data_random_seed': 1337,\n",
    "        'objective': 'binary',\n",
    "        'boosting_type': 'dart',\n",
    "        'verbose': 1,\n",
    "        'is_unbalance': True,\n",
    "        'boost_from_average': False,\n",
    "        'device': 'gpu',\n",
    "        'gpu_platform_id': 0,\n",
    "        'gpu_device_id': 0\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version 02 - Kaggle Metric XGB - 0.7466903183087572\n",
    "* Dropped features from spearman correlation\n",
    "\n",
    "### Version 03 - Kaggle Metric XGB - 0.7399987922099773\n",
    "* Drop features from WOE and IV \n",
    "* Feature drop with no knowledge about feature meaning is a problem.\n",
    "\n",
    "### Version 04 - Kaggle Metric LGBM - \n",
    "* Force features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "### Fold 1\n",
      "### Train size 367130 Valid size 91783\n",
      "### Training with 100% fold data...\n",
      "#########################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\brito\\OneDrive\\Documentos\\1 - Data Science\\3 - Projeto\\04 - American Express\\venv\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "c:\\Users\\brito\\OneDrive\\Documentos\\1 - Data Science\\3 - Projeto\\04 - American Express\\venv\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 95285, number of negative: 271845\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 79515\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 913\n",
      "[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 482 dense feature groups (169.46 MB) transferred to GPU in 0.069656 secs. 1 sparse feature groups\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\brito\\OneDrive\\Documentos\\1 - Data Science\\3 - Projeto\\04 - American Express\\venv\\lib\\site-packages\\lightgbm\\callback.py:223: UserWarning: Early stopping is not available in dart mode\n",
      "  _log_warning('Early stopping is not available in dart mode')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500]\ttraining's auc: 0.950668\ttraining's amex_metric: 0.744009\tvalid_1's auc: 0.949473\tvalid_1's amex_metric: 0.740578\n",
      "[1000]\ttraining's auc: 0.955238\ttraining's amex_metric: 0.763613\tvalid_1's auc: 0.953793\tvalid_1's amex_metric: 0.760231\n"
     ]
    }
   ],
   "source": [
    "importances = []\n",
    "oof = []\n",
    "TRAIN_SUBSAMPLE = 1.0\n",
    "\n",
    "skf = KFold(n_splits = FOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "for fold, (train_idx, valid_idx) in enumerate(skf.split(train, train.target)):\n",
    "\n",
    "    if TRAIN_SUBSAMPLE<1.0:\n",
    "        np.random.seed(SEED)\n",
    "        train_idx = np.random.choice(train_idx, \n",
    "                       int(len(train_idx)*TRAIN_SUBSAMPLE), replace=False)\n",
    "        np.random.seed(None)\n",
    "        \n",
    "    print('#'*25)\n",
    "    print('### Fold',fold+1)\n",
    "    print('### Train size',len(train_idx),'Valid size',len(valid_idx))\n",
    "    print(f'### Training with {int(TRAIN_SUBSAMPLE*100)}% fold data...')\n",
    "    print('#'*25)\n",
    "\n",
    "    X_train = train.loc[train_idx, FEATURES]\n",
    "    y_train = train.loc[train_idx, 'target']\n",
    "    X_valid = train.loc[valid_idx, FEATURES]\n",
    "    y_valid = train.loc[valid_idx, 'target']\n",
    "\n",
    "    dtrain = lgb.Dataset(X_train, y_train)\n",
    "    dvalid= lgb.Dataset(X_valid, y_valid)\n",
    "    \n",
    "    model = lgb.train(\n",
    "                    params= xgb_params,\n",
    "                    train_set=dtrain,\n",
    "                    valid_sets=[dtrain, dvalid],\n",
    "                    num_boost_round= 9999,\n",
    "                    early_stopping_rounds = 100,\n",
    "                    verbose_eval= 500,\n",
    "                    feval = lgb_amex_metric\n",
    "                    )\n",
    "\n",
    "    model.save_model(f'../models/LGB_V{VER}_fold{fold}.lgb')\n",
    "\n",
    "    oof_preds = model.predict(X_valid)\n",
    "    acc = amex_metric(y_valid.values, oof_preds)\n",
    "    print(\"Kaggle Metric=\", acc,'\\n')\n",
    "\n",
    "    df = train.loc[valid_idx, ['customer_ID', 'target']].copy()\n",
    "    df['oof_pred']= oof_preds\n",
    "    oof.append(df)\n",
    "\n",
    "    del dtrain, X_train, y_train, df\n",
    "    del X_valid, y_valid, dvalid, model\n",
    "\n",
    "print('#'*25)\n",
    "oof = pd.concat(oof, axis=0, ignore_index=True).set_index('customer_ID')\n",
    "acc= amex_metric(oof.target.values, oof.oof_pred.values)\n",
    "print('OVERAL CV Kaggle Metric = ', acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
