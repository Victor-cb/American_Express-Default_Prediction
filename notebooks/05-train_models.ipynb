{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "import matplotlib.pyplot as plt, gc, os\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import mlflow.xgboost\n",
    "import mlflow.lightgbm\n",
    "from xgboost import XGBClassifier\n",
    "import pickle\n",
    "import warnings\n",
    "import xgboost as xgb\n",
    "import time\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = [\n",
    "    \"B_30\",\n",
    "    \"B_38\",\n",
    "    \"D_114\",\n",
    "    \"D_116\",\n",
    "    \"D_117\",\n",
    "    \"D_120\",\n",
    "    \"D_126\",\n",
    "    \"D_63\",\n",
    "    \"D_64\",\n",
    "    \"D_66\",\n",
    "    \"D_68\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = \"03\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Woe_balanced dataframe\n",
    "train = pd.read_parquet(\"../data/final/train_full_woebalanced.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "iv_features = pd.read_csv(\"../reports/iv_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.drop('WOE_target',axis=1, inplace=True)\n",
    "FEATURES = iv_features[\"useful\"].to_list()\n",
    "FEATURES.remove(\"target\")\n",
    "FEATURES = FEATURES[:-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Competition metric\n",
    "def amex_metric(y_true, y_pred):\n",
    "\n",
    "    labels = np.transpose(np.array([y_true, y_pred]))\n",
    "    labels = labels[labels[:, 1].argsort()[::-1]]\n",
    "    weights = np.where(labels[:, 0] == 0, 20, 1)\n",
    "    cut_vals = labels[np.cumsum(weights) <= int(0.04 * np.sum(weights))]\n",
    "    top_four = np.sum(cut_vals[:, 0]) / np.sum(labels[:, 0])\n",
    "\n",
    "    gini = [0, 0]\n",
    "    for i in [1, 0]:\n",
    "        labels = np.transpose(np.array([y_true, y_pred]))\n",
    "        labels = labels[labels[:, i].argsort()[::-1]]\n",
    "        weight = np.where(labels[:, 0] == 0, 20, 1)\n",
    "        weight_random = np.cumsum(weight / np.sum(weight))\n",
    "        total_pos = np.sum(labels[:, 0] * weight)\n",
    "        cum_pos_found = np.cumsum(labels[:, 0] * weight)\n",
    "        lorentz = cum_pos_found / total_pos\n",
    "        gini[i] = np.sum((lorentz - weight_random) * weight)\n",
    "\n",
    "    return 0.5 * (gini[1] / gini[0] + top_four)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XgBoost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Parameters\n",
    "\n",
    "xgb_params = {\n",
    "    \"max_depth\": 4,\n",
    "    \"learning_rate\": 0.045,\n",
    "    #'max_delta_step':3,\n",
    "    \"subsample\": 0.6,\n",
    "    \"sampling_method\": \"gradient_based\",\n",
    "    # \"lambda\": 0.7,\n",
    "    \"alpha\": 0.8,\n",
    "    \"tree_method\": \"gpu_hist\",\n",
    "    # \"scale_pos_weight\": 0.333,\n",
    "    # \"max_bin\": 20,\n",
    "    \"colsample_bytree\": 0.6,\n",
    "    \"eval_metric\": \"logloss\",\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"predictor\": \"gpu_predictor\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting MLFlow\n",
    "experiment_name = \"XGBoost - WoE Balanced + IV Balanced\"\n",
    "try:\n",
    "    exp_id = mlflow.create_experiment(name=experiment_name)\n",
    "except Exception as e:\n",
    "    exp_id = mlflow.get_experiment_by_name(experiment_name).experiment_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/10/26 17:10:50 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.\n",
      "2022/10/26 17:10:50 INFO mlflow.tracking.fluent: Autologging successfully enabled for xgboost.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "### Fold 1\n",
      "### Train size 4425160 Valid size 1106291\n",
      "### Training with 100% fold data...\n",
      "#########################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/10/26 17:10:57 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID 'b48e58e6504749dda1f9164ce593f8e9', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current xgboost workflow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.67437\ttest-logloss:0.67435\n",
      "[100]\ttrain-logloss:0.35533\ttest-logloss:0.35495\n",
      "[200]\ttrain-logloss:0.33384\ttest-logloss:0.33352\n",
      "[300]\ttrain-logloss:0.32712\ttest-logloss:0.32687\n",
      "[400]\ttrain-logloss:0.32397\ttest-logloss:0.32380\n",
      "[500]\ttrain-logloss:0.32179\ttest-logloss:0.32169\n",
      "[600]\ttrain-logloss:0.32015\ttest-logloss:0.32013\n",
      "[700]\ttrain-logloss:0.31883\ttest-logloss:0.31888\n",
      "[800]\ttrain-logloss:0.31759\ttest-logloss:0.31769\n",
      "[900]\ttrain-logloss:0.31664\ttest-logloss:0.31680\n",
      "[1000]\ttrain-logloss:0.31568\ttest-logloss:0.31592\n",
      "[1100]\ttrain-logloss:0.31490\ttest-logloss:0.31521\n",
      "[1200]\ttrain-logloss:0.31425\ttest-logloss:0.31462\n",
      "[1300]\ttrain-logloss:0.31357\ttest-logloss:0.31400\n",
      "[1400]\ttrain-logloss:0.31297\ttest-logloss:0.31348\n",
      "[1500]\ttrain-logloss:0.31244\ttest-logloss:0.31301\n",
      "[1600]\ttrain-logloss:0.31196\ttest-logloss:0.31258\n",
      "[1700]\ttrain-logloss:0.31152\ttest-logloss:0.31220\n",
      "[1800]\ttrain-logloss:0.31111\ttest-logloss:0.31185\n",
      "[1900]\ttrain-logloss:0.31068\ttest-logloss:0.31149\n",
      "[2000]\ttrain-logloss:0.31034\ttest-logloss:0.31122\n",
      "[2100]\ttrain-logloss:0.30997\ttest-logloss:0.31090\n",
      "[2200]\ttrain-logloss:0.30962\ttest-logloss:0.31061\n",
      "[2300]\ttrain-logloss:0.30929\ttest-logloss:0.31033\n",
      "[2400]\ttrain-logloss:0.30899\ttest-logloss:0.31009\n",
      "[2500]\ttrain-logloss:0.30872\ttest-logloss:0.30988\n",
      "[2600]\ttrain-logloss:0.30847\ttest-logloss:0.30968\n",
      "[2700]\ttrain-logloss:0.30822\ttest-logloss:0.30949\n",
      "[2800]\ttrain-logloss:0.30796\ttest-logloss:0.30929\n",
      "[2900]\ttrain-logloss:0.30771\ttest-logloss:0.30909\n",
      "[3000]\ttrain-logloss:0.30744\ttest-logloss:0.30888\n",
      "[3100]\ttrain-logloss:0.30721\ttest-logloss:0.30870\n",
      "[3200]\ttrain-logloss:0.30699\ttest-logloss:0.30852\n",
      "[3300]\ttrain-logloss:0.30676\ttest-logloss:0.30835\n",
      "[3400]\ttrain-logloss:0.30654\ttest-logloss:0.30818\n",
      "[3500]\ttrain-logloss:0.30631\ttest-logloss:0.30800\n",
      "[3600]\ttrain-logloss:0.30610\ttest-logloss:0.30786\n",
      "[3700]\ttrain-logloss:0.30590\ttest-logloss:0.30770\n",
      "[3800]\ttrain-logloss:0.30570\ttest-logloss:0.30754\n",
      "[3900]\ttrain-logloss:0.30549\ttest-logloss:0.30738\n",
      "[4000]\ttrain-logloss:0.30530\ttest-logloss:0.30725\n",
      "[4100]\ttrain-logloss:0.30511\ttest-logloss:0.30711\n",
      "[4200]\ttrain-logloss:0.30491\ttest-logloss:0.30696\n",
      "[4300]\ttrain-logloss:0.30473\ttest-logloss:0.30683\n",
      "[4400]\ttrain-logloss:0.30456\ttest-logloss:0.30671\n",
      "[4500]\ttrain-logloss:0.30438\ttest-logloss:0.30659\n",
      "[4600]\ttrain-logloss:0.30422\ttest-logloss:0.30647\n",
      "[4700]\ttrain-logloss:0.30405\ttest-logloss:0.30635\n",
      "[4800]\ttrain-logloss:0.30387\ttest-logloss:0.30622\n",
      "[4900]\ttrain-logloss:0.30371\ttest-logloss:0.30611\n",
      "[5000]\ttrain-logloss:0.30353\ttest-logloss:0.30598\n",
      "[5100]\ttrain-logloss:0.30335\ttest-logloss:0.30585\n",
      "[5200]\ttrain-logloss:0.30319\ttest-logloss:0.30574\n",
      "[5300]\ttrain-logloss:0.30302\ttest-logloss:0.30561\n",
      "[5400]\ttrain-logloss:0.30284\ttest-logloss:0.30549\n",
      "[5500]\ttrain-logloss:0.30268\ttest-logloss:0.30537\n",
      "[5600]\ttrain-logloss:0.30252\ttest-logloss:0.30527\n",
      "[5700]\ttrain-logloss:0.30237\ttest-logloss:0.30517\n",
      "[5800]\ttrain-logloss:0.30223\ttest-logloss:0.30507\n",
      "[5900]\ttrain-logloss:0.30208\ttest-logloss:0.30497\n",
      "[6000]\ttrain-logloss:0.30192\ttest-logloss:0.30485\n",
      "[6100]\ttrain-logloss:0.30176\ttest-logloss:0.30474\n",
      "[6200]\ttrain-logloss:0.30160\ttest-logloss:0.30463\n",
      "[6300]\ttrain-logloss:0.30145\ttest-logloss:0.30453\n",
      "[6400]\ttrain-logloss:0.30131\ttest-logloss:0.30443\n",
      "[6500]\ttrain-logloss:0.30117\ttest-logloss:0.30434\n",
      "[6600]\ttrain-logloss:0.30104\ttest-logloss:0.30425\n",
      "[6700]\ttrain-logloss:0.30091\ttest-logloss:0.30416\n",
      "[6800]\ttrain-logloss:0.30077\ttest-logloss:0.30407\n",
      "[6900]\ttrain-logloss:0.30063\ttest-logloss:0.30398\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1147839/1168577748.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mX_valid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     model = xgb.train(\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mxgb_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mdtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/amex/lib/python3.8/site-packages/mlflow/utils/autologging_utils/safety.py\u001b[0m in \u001b[0;36msafe_patch_function\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    553\u001b[0m                         \u001b[0mpatch_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_original\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m                         \u001b[0mpatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_original\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m                     \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"succeeded\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/amex/lib/python3.8/site-packages/mlflow/utils/autologging_utils/safety.py\u001b[0m in \u001b[0;36mpatch_with_managed_run\u001b[0;34m(original, *args, **kwargs)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mException\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m                 \u001b[0;31m# In addition to standard Python exceptions, handle keyboard interrupts to ensure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/amex/lib/python3.8/site-packages/mlflow/xgboost/__init__.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(_log_models, original, *args, **kwargs)\u001b[0m\n\u001b[1;32m    605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m             \u001b[0;31m# training model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 607\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moriginal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    608\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m             \u001b[0;31m# If early_stopping_rounds is present, logging metrics at the best iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/amex/lib/python3.8/site-packages/mlflow/utils/autologging_utils/safety.py\u001b[0m in \u001b[0;36mcall_original\u001b[0;34m(*og_args, **og_kwargs)\u001b[0m\n\u001b[1;32m    534\u001b[0m                                 \u001b[0;32mreturn\u001b[0m \u001b[0moriginal_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 536\u001b[0;31m                         \u001b[0;32mreturn\u001b[0m \u001b[0mcall_original_fn_with_event_logging\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_original_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mog_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mog_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m                     \u001b[0;31m# Apply the name, docstring, and signature of `original` to `call_original`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/amex/lib/python3.8/site-packages/mlflow/utils/autologging_utils/safety.py\u001b[0m in \u001b[0;36mcall_original_fn_with_event_logging\u001b[0;34m(original_fn, og_args, og_kwargs)\u001b[0m\n\u001b[1;32m    469\u001b[0m                         \u001b[0mog_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m                     )\n\u001b[0;32m--> 471\u001b[0;31m                     \u001b[0moriginal_fn_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moriginal_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mog_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mog_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m                     try_log_autologging_event(\n",
      "\u001b[0;32m~/miniconda3/envs/amex/lib/python3.8/site-packages/mlflow/utils/autologging_utils/safety.py\u001b[0m in \u001b[0;36m_original_fn\u001b[0;34m(*_og_args, **_og_kwargs)\u001b[0m\n\u001b[1;32m    531\u001b[0m                                 \u001b[0mreroute_warnings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m                             ):\n\u001b[0;32m--> 533\u001b[0;31m                                 \u001b[0moriginal_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moriginal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0m_og_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0m_og_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m                                 \u001b[0;32mreturn\u001b[0m \u001b[0moriginal_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/amex/lib/python3.8/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    573\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/amex/lib/python3.8/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbefore_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/amex/lib/python3.8/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1778\u001b[0;31m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,\n\u001b[0m\u001b[1;32m   1779\u001b[0m                                                     \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1780\u001b[0m                                                     dtrain.handle))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mlflow.autolog()\n",
    "importances = []\n",
    "oof = []\n",
    "TRAIN_SUBSAMPLE = 1.0\n",
    "\n",
    "skf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for fold, (train_idx, valid_idx) in enumerate(skf.split(train, train.target)):\n",
    "\n",
    "    if TRAIN_SUBSAMPLE < 1.0:\n",
    "        np.random.seed(42)\n",
    "        train_idx = np.random.choice(\n",
    "            train_idx, int(len(train_idx) * TRAIN_SUBSAMPLE), replace=False\n",
    "        )\n",
    "        np.random.seed(None)\n",
    "\n",
    "    print(\"#\" * 25)\n",
    "    print(\"### Fold\", fold + 1)\n",
    "    print(\"### Train size\", len(train_idx), \"Valid size\", len(valid_idx))\n",
    "    print(f\"### Training with {int(TRAIN_SUBSAMPLE*100)}% fold data...\")\n",
    "    print(\"#\" * 25)\n",
    "\n",
    "    X_train = train.loc[train_idx, FEATURES]\n",
    "    y_train = train.loc[train_idx, \"target\"]\n",
    "    X_valid = train.loc[valid_idx, FEATURES]\n",
    "    y_valid = train.loc[valid_idx, \"target\"]\n",
    "\n",
    "    dtrain = xgb.DMatrix(X_train, y_train)\n",
    "    del X_train, y_train\n",
    "    gc.collect()\n",
    "    d_valid = xgb.DMatrix(X_valid, y_valid)\n",
    "    del X_valid\n",
    "    gc.collect()\n",
    "    model = xgb.train(\n",
    "        xgb_params,\n",
    "        dtrain=dtrain,\n",
    "        evals=[(dtrain, \"train\"), (d_valid, \"test\")],\n",
    "        num_boost_round=9999,\n",
    "        early_stopping_rounds=100,\n",
    "        verbose_eval=100,\n",
    "    )\n",
    "\n",
    "    model.save_model(f\"../models/XGB_Version{VERSION}_fold{fold}.xgb\")\n",
    "    mlflow.xgboost.log_model(model, \"XGBClassifier\")\n",
    "\n",
    "    dd = model.get_score(importance_type=\"weight\")\n",
    "    df = pd.DataFrame({\"feature\": dd.keys(), f\"importance_{fold}\": dd.values()})\n",
    "    importances.append(df)\n",
    "\n",
    "    oof_preds = model.predict(d_valid)\n",
    "    acc = amex_metric(y_valid.values, oof_preds)\n",
    "    mlflow.log_metric(\"Kaggle Metric for XGBClassifier\", acc)\n",
    "\n",
    "    print(\"Kaggle Metric=\", acc, \"\\n\")\n",
    "\n",
    "    df = train.loc[valid_idx, [\"customer_ID\", \"target\"]].copy()\n",
    "    df[\"oof_pred\"] = oof_preds\n",
    "    oof.append(df)\n",
    "\n",
    "    del dd, df\n",
    "    del d_valid, model\n",
    "    gc.collect()\n",
    "print(\"#\" * 25)\n",
    "oof = pd.concat(oof, axis=0, ignore_index=True).set_index(\"customer_ID\")\n",
    "acc = amex_metric(oof.target.values, oof.oof_pred.values)\n",
    "print(\"OVERAL CV Kaggle Metric = \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.system('systemctl poweroff')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/10/10 22:45:00 INFO mlflow.tracking.fluent: Autologging successfully enabled for xgboost.\n",
      "2022/10/10 22:45:01 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.\n"
     ]
    }
   ],
   "source": [
    "# Setting MLFlow\n",
    "experiment_name = \"RegLog - WoE Balanced + IV Balanced\"\n",
    "try:\n",
    "    exp_id = mlflow.create_experiment(name=experiment_name)\n",
    "except Exception as e:\n",
    "    exp_id = mlflow.get_experiment_by_name(experiment_name).experiment_id\n",
    "mlflow.autolog()\n",
    "importances = []\n",
    "oof = []\n",
    "TRAIN_SUBSAMPLE = 1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "### Fold 1\n",
      "### Train size 4425160 Valid size 1106291\n",
      "### Training with 100% fold data...\n",
      "#########################\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'LogisticRegression' object has no attribute 'save_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_78311/1078334253.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"../models/RegLog_{VERSION}_fold{fold}.xgb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mmlflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"RegLog\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LogisticRegression' object has no attribute 'save_model'"
     ]
    }
   ],
   "source": [
    "\n",
    "skf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for fold, (train_idx, valid_idx) in enumerate(skf.split(train, train.target)):\n",
    "\n",
    "    if TRAIN_SUBSAMPLE < 1.0:\n",
    "        np.random.seed(42)\n",
    "        train_idx = np.random.choice(\n",
    "            train_idx, int(len(train_idx) * TRAIN_SUBSAMPLE), replace=False\n",
    "        )\n",
    "        np.random.seed(None)\n",
    "\n",
    "    print(\"#\" * 25)\n",
    "    print(\"### Fold\", fold + 1)\n",
    "    print(\"### Train size\", len(train_idx), \"Valid size\", len(valid_idx))\n",
    "    print(f\"### Training with {int(TRAIN_SUBSAMPLE*100)}% fold data...\")\n",
    "    print(\"#\" * 25)\n",
    "\n",
    "    X_train = train.loc[train_idx, FEATURES]\n",
    "    y_train = train.loc[train_idx, \"target\"]\n",
    "    X_valid = train.loc[valid_idx, FEATURES]\n",
    "    y_valid = train.loc[valid_idx, \"target\"]\n",
    "\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    model.save_model(f\"../models/RegLog_{VERSION}_fold{fold}.xgb\")\n",
    "    mlflow.sklearn.log_model(model, \"RegLog\")\n",
    "\n",
    "    #dd = model.get_score(importance_type=\"weight\")\n",
    "    df = pd.DataFrame({\"feature\": dd.keys(), f\"importance_{fold}\": dd.values()})\n",
    "    importances.append(df)\n",
    "\n",
    "    oof_preds = model.predict(d_valid)\n",
    "    acc = amex_metric(y_valid.values, oof_preds)\n",
    "    mlflow.log_metric(\"Kaggle Metric for RegLog\", acc)\n",
    "\n",
    "    print(\"Kaggle Metric=\", acc, \"\\n\")\n",
    "\n",
    "    df = train.loc[valid_idx, [\"customer_ID\", \"target\"]].copy()\n",
    "    df[\"oof_pred\"] = oof_preds\n",
    "    oof.append(df)\n",
    "\n",
    "    del df\n",
    "    del d_valid, model\n",
    "    gc.collect()\n",
    "print(\"#\" * 25)\n",
    "oof = pd.concat(oof, axis=0, ignore_index=True).set_index(\"customer_ID\")\n",
    "acc = amex_metric(oof.target.values, oof.oof_pred.values)\n",
    "print(\"OVERAL CV Kaggle Metric = \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('amex')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "2402223ceb02c82e09767fed984839ab3646589a63bcb32db44ba9d92921d1b9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
