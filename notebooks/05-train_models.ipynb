{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "import matplotlib.pyplot as plt, gc, os\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import mlflow.xgboost\n",
    "import mlflow.lightgbm\n",
    "from xgboost import XGBClassifier\n",
    "import pickle\n",
    "import warnings \n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import time\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Woe_balanced dataframe\n",
    "train = pd.read_parquet(\"../data/train_woe_balanced_downcast.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "iv_features = pd.read_csv(\"../data/iv_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D_39',\n",
       " 'D_41',\n",
       " 'D_43',\n",
       " 'B_5',\n",
       " 'R_2',\n",
       " 'D_46',\n",
       " 'B_8',\n",
       " 'D_50',\n",
       " 'R_3',\n",
       " 'P_3',\n",
       " 'D_53',\n",
       " 'S_5',\n",
       " 'S_6',\n",
       " 'R_4',\n",
       " 'B_12',\n",
       " 'S_8',\n",
       " 'D_56',\n",
       " 'B_13',\n",
       " 'S_9',\n",
       " 'D_59',\n",
       " 'D_60',\n",
       " 'S_11',\n",
       " 'D_63',\n",
       " 'D_64',\n",
       " 'D_68',\n",
       " 'S_12',\n",
       " 'R_6',\n",
       " 'S_13',\n",
       " 'B_21',\n",
       " 'D_69',\n",
       " 'D_70',\n",
       " 'D_71',\n",
       " 'D_72',\n",
       " 'S_15',\n",
       " 'P_4',\n",
       " 'D_76',\n",
       " 'B_24',\n",
       " 'B_26',\n",
       " 'D_78',\n",
       " 'D_79',\n",
       " 'S_16',\n",
       " 'D_81',\n",
       " 'D_82',\n",
       " 'D_83',\n",
       " 'R_15',\n",
       " 'D_84',\n",
       " 'D_86',\n",
       " 'R_19',\n",
       " 'B_32',\n",
       " 'S_20',\n",
       " 'R_20',\n",
       " 'R_21',\n",
       " 'D_89',\n",
       " 'D_91',\n",
       " 'D_92',\n",
       " 'D_94',\n",
       " 'R_24',\n",
       " 'D_96',\n",
       " 'S_23',\n",
       " 'S_25',\n",
       " 'S_26',\n",
       " 'D_102',\n",
       " 'D_103',\n",
       " 'D_104',\n",
       " 'D_105',\n",
       " 'D_107',\n",
       " 'R_26',\n",
       " 'R_27',\n",
       " 'D_112',\n",
       " 'S_27',\n",
       " 'D_113',\n",
       " 'D_114',\n",
       " 'D_115',\n",
       " 'D_116',\n",
       " 'D_117',\n",
       " 'D_118',\n",
       " 'D_119',\n",
       " 'D_120',\n",
       " 'D_121',\n",
       " 'D_122',\n",
       " 'D_124',\n",
       " 'D_125',\n",
       " 'D_126',\n",
       " 'D_127',\n",
       " 'D_128',\n",
       " 'D_129',\n",
       " 'B_41',\n",
       " 'B_42',\n",
       " 'D_130',\n",
       " 'D_132',\n",
       " 'D_133',\n",
       " 'D_134',\n",
       " 'D_135',\n",
       " 'D_136',\n",
       " 'D_137',\n",
       " 'D_138',\n",
       " 'D_139',\n",
       " 'D_140',\n",
       " 'D_141',\n",
       " 'D_142',\n",
       " 'D_143',\n",
       " 'D_145']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train.drop('WOE_target',axis=1, inplace=True)\n",
    "FEATURES = iv_features[\"useful\"].to_list()\n",
    "FEATURES.remove(\"target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Competition metric\n",
    "def amex_metric(y_true, y_pred):\n",
    "\n",
    "    labels     = np.transpose(np.array([y_true, y_pred]))\n",
    "    labels     = labels[labels[:, 1].argsort()[::-1]]\n",
    "    weights    = np.where(labels[:,0]==0, 20, 1)\n",
    "    cut_vals   = labels[np.cumsum(weights) <= int(0.04 * np.sum(weights))]\n",
    "    top_four   = np.sum(cut_vals[:,0]) / np.sum(labels[:,0])\n",
    "\n",
    "    gini = [0,0]\n",
    "    for i in [1,0]:\n",
    "        labels         = np.transpose(np.array([y_true, y_pred]))\n",
    "        labels         = labels[labels[:, i].argsort()[::-1]]\n",
    "        weight         = np.where(labels[:,0]==0, 20, 1)\n",
    "        weight_random  = np.cumsum(weight / np.sum(weight))\n",
    "        total_pos      = np.sum(labels[:, 0] *  weight)\n",
    "        cum_pos_found  = np.cumsum(labels[:, 0] * weight)\n",
    "        lorentz        = cum_pos_found / total_pos\n",
    "        gini[i]        = np.sum((lorentz - weight_random) * weight)\n",
    "\n",
    "    return 0.5 * (gini[1]/gini[0] + top_four)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Parameters\n",
    "\n",
    "xgb_params = {\n",
    "        'max_depth': 20,\n",
    "        'learning_rate':0.05,\n",
    "        'max_delta_step':3,\n",
    "        'subsample':0.6,\n",
    "        'sampling_method':'gradient_based',\n",
    "        'lambda':0.8,\n",
    "        'alpha':0.8,\n",
    "        'tree_method':'gpu_hist',\n",
    "        'scale_pos_weight':0.3317302992934773,\n",
    "        'max_bin':20,\n",
    "        'colsample_bytree':0.6, \n",
    "        'eval_metric':'logloss',\n",
    "        'objective':'binary:logistic',\n",
    "        'predictor':'auto',\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XgBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting MLFlow\n",
    "experiment_name = \"XGBoost - WoE Balanced + IV Balanced\"\n",
    "try:\n",
    "    exp_id = mlflow.create_experiment(name=experiment_name)\n",
    "except Exception as e:\n",
    "    exp_id = mlflow.get_experiment_by_name(experiment_name).experiment_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/09/30 13:56:05 INFO mlflow.tracking.fluent: Autologging successfully enabled for xgboost.\n",
      "2022/09/30 13:56:05 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.\n",
      "2022/09/30 13:56:05 INFO mlflow.tracking.fluent: Autologging successfully enabled for lightgbm.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "### Fold 1\n",
      "### Train size 4425160 Valid size 1106291\n",
      "### Training with 100% fold data...\n",
      "#########################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/09/30 13:56:13 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID 'c5bf80da3b3b495699e8165c02873152', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current xgboost workflow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.66611\ttest-logloss:0.66730\n",
      "[100]\ttrain-logloss:0.22298\ttest-logloss:0.29632\n",
      "[200]\ttrain-logloss:0.17260\ttest-logloss:0.26810\n",
      "[300]\ttrain-logloss:0.13489\ttest-logloss:0.24888\n",
      "[400]\ttrain-logloss:0.10641\ttest-logloss:0.23391\n",
      "[500]\ttrain-logloss:0.08708\ttest-logloss:0.22371\n",
      "[600]\ttrain-logloss:0.07266\ttest-logloss:0.21630\n",
      "[700]\ttrain-logloss:0.06127\ttest-logloss:0.21105\n",
      "[800]\ttrain-logloss:0.05249\ttest-logloss:0.20670\n",
      "[900]\ttrain-logloss:0.04561\ttest-logloss:0.20351\n",
      "[1000]\ttrain-logloss:0.04044\ttest-logloss:0.20081\n",
      "[1100]\ttrain-logloss:0.03616\ttest-logloss:0.19894\n",
      "[1200]\ttrain-logloss:0.03265\ttest-logloss:0.19729\n",
      "[1300]\ttrain-logloss:0.02981\ttest-logloss:0.19609\n",
      "[1400]\ttrain-logloss:0.02732\ttest-logloss:0.19494\n",
      "[1500]\ttrain-logloss:0.02523\ttest-logloss:0.19394\n",
      "[1600]\ttrain-logloss:0.02356\ttest-logloss:0.19304\n",
      "[1700]\ttrain-logloss:0.02201\ttest-logloss:0.19247\n",
      "[1800]\ttrain-logloss:0.02068\ttest-logloss:0.19179\n",
      "[1900]\ttrain-logloss:0.01950\ttest-logloss:0.19135\n",
      "[2000]\ttrain-logloss:0.01846\ttest-logloss:0.19095\n",
      "[2100]\ttrain-logloss:0.01753\ttest-logloss:0.19054\n",
      "[2200]\ttrain-logloss:0.01673\ttest-logloss:0.19024\n",
      "[2300]\ttrain-logloss:0.01597\ttest-logloss:0.18992\n",
      "[2400]\ttrain-logloss:0.01530\ttest-logloss:0.18960\n",
      "[2500]\ttrain-logloss:0.01471\ttest-logloss:0.18931\n",
      "[2600]\ttrain-logloss:0.01414\ttest-logloss:0.18911\n",
      "[2700]\ttrain-logloss:0.01362\ttest-logloss:0.18887\n",
      "[2800]\ttrain-logloss:0.01317\ttest-logloss:0.18864\n",
      "[2900]\ttrain-logloss:0.01274\ttest-logloss:0.18848\n",
      "[3000]\ttrain-logloss:0.01234\ttest-logloss:0.18835\n",
      "[3100]\ttrain-logloss:0.01199\ttest-logloss:0.18823\n",
      "[3200]\ttrain-logloss:0.01164\ttest-logloss:0.18813\n",
      "[3300]\ttrain-logloss:0.01133\ttest-logloss:0.18791\n",
      "[3400]\ttrain-logloss:0.01104\ttest-logloss:0.18781\n",
      "[3500]\ttrain-logloss:0.01076\ttest-logloss:0.18777\n",
      "[3600]\ttrain-logloss:0.01049\ttest-logloss:0.18759\n",
      "[3700]\ttrain-logloss:0.01024\ttest-logloss:0.18752\n",
      "[3800]\ttrain-logloss:0.00999\ttest-logloss:0.18747\n",
      "[3900]\ttrain-logloss:0.00978\ttest-logloss:0.18741\n",
      "[4000]\ttrain-logloss:0.00957\ttest-logloss:0.18733\n",
      "[4100]\ttrain-logloss:0.00938\ttest-logloss:0.18728\n",
      "[4200]\ttrain-logloss:0.00919\ttest-logloss:0.18720\n",
      "[4246]\ttrain-logloss:0.00910\ttest-logloss:0.18721\n"
     ]
    },
    {
     "ename": "XGBoostError",
     "evalue": "[15:24:39] ../src/c_api/../data/../common/device_helpers.cuh:428: Memory allocation error on worker 0: std::bad_alloc: cudaErrorMemoryAllocation: out of memory\n- Free memory: 499646464\n- Requested memory: 913359440\n\nStack trace:\n  [bt] (0) /home/victor/Documents/1-DataScience/1-Projetos/American_Express-Default_Prediction/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x3a5799) [0x7f060f33c799]\n  [bt] (1) /home/victor/Documents/1-DataScience/1-Projetos/American_Express-Default_Prediction/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x3a9bab) [0x7f060f340bab]\n  [bt] (2) /home/victor/Documents/1-DataScience/1-Projetos/American_Express-Default_Prediction/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x8da20) [0x7f060f024a20]\n  [bt] (3) /home/victor/Documents/1-DataScience/1-Projetos/American_Express-Default_Prediction/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x3f9337) [0x7f060f390337]\n  [bt] (4) /home/victor/Documents/1-DataScience/1-Projetos/American_Express-Default_Prediction/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x3fa333) [0x7f060f391333]\n  [bt] (5) /home/victor/Documents/1-DataScience/1-Projetos/American_Express-Default_Prediction/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x5d59de) [0x7f060f56c9de]\n  [bt] (6) /home/victor/Documents/1-DataScience/1-Projetos/American_Express-Default_Prediction/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x5d9384) [0x7f060f570384]\n  [bt] (7) /home/victor/Documents/1-DataScience/1-Projetos/American_Express-Default_Prediction/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x5daff5) [0x7f060f571ff5]\n  [bt] (8) /home/victor/Documents/1-DataScience/1-Projetos/American_Express-Default_Prediction/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1e5aa5) [0x7f060f17caa5]\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mXGBoostError\u001b[0m                              Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn [10], line 52\u001b[0m\n",
      "\u001b[1;32m     49\u001b[0m df\u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature\u001b[39m\u001b[38;5;124m'\u001b[39m:dd\u001b[38;5;241m.\u001b[39mkeys(), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimportance_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m:dd\u001b[38;5;241m.\u001b[39mvalues()})\n",
      "\u001b[1;32m     50\u001b[0m importances\u001b[38;5;241m.\u001b[39mappend(df)\n",
      "\u001b[0;32m---> 52\u001b[0m oof_preds \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(d_valid)\n",
      "\u001b[1;32m     53\u001b[0m acc \u001b[38;5;241m=\u001b[39m amex_metric(y_valid\u001b[38;5;241m.\u001b[39mvalues, oof_preds)\n",
      "\u001b[1;32m     54\u001b[0m mlflow\u001b[38;5;241m.\u001b[39mlog_metric(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKaggle Metric for XGBClassifier\u001b[39m\u001b[38;5;124m\"\u001b[39m, acc)\n",
      "\n",
      "File \u001b[0;32m~/Documents/1-DataScience/1-Projetos/American_Express-Default_Prediction/venv/lib/python3.8/site-packages/xgboost/core.py:2023\u001b[0m, in \u001b[0;36mBooster.predict\u001b[0;34m(self, data, output_margin, ntree_limit, pred_leaf, pred_contribs, approx_contribs, pred_interactions, validate_features, training, iteration_range, strict_shape)\u001b[0m\n",
      "\u001b[1;32m   2021\u001b[0m shape \u001b[39m=\u001b[39m ctypes\u001b[39m.\u001b[39mPOINTER(c_bst_ulong)()\n",
      "\u001b[1;32m   2022\u001b[0m dims \u001b[39m=\u001b[39m c_bst_ulong()\n",
      "\u001b[0;32m-> 2023\u001b[0m _check_call(\n",
      "\u001b[1;32m   2024\u001b[0m     _LIB\u001b[39m.\u001b[39;49mXGBoosterPredictFromDMatrix(\n",
      "\u001b[1;32m   2025\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle,\n",
      "\u001b[1;32m   2026\u001b[0m         data\u001b[39m.\u001b[39;49mhandle,\n",
      "\u001b[1;32m   2027\u001b[0m         from_pystr_to_cstr(json\u001b[39m.\u001b[39;49mdumps(args)),\n",
      "\u001b[1;32m   2028\u001b[0m         ctypes\u001b[39m.\u001b[39;49mbyref(shape),\n",
      "\u001b[1;32m   2029\u001b[0m         ctypes\u001b[39m.\u001b[39;49mbyref(dims),\n",
      "\u001b[1;32m   2030\u001b[0m         ctypes\u001b[39m.\u001b[39;49mbyref(preds)\n",
      "\u001b[1;32m   2031\u001b[0m     )\n",
      "\u001b[1;32m   2032\u001b[0m )\n",
      "\u001b[1;32m   2033\u001b[0m \u001b[39mreturn\u001b[39;00m _prediction_output(shape, dims, preds, \u001b[39mFalse\u001b[39;00m)\n",
      "\n",
      "File \u001b[0;32m~/Documents/1-DataScience/1-Projetos/American_Express-Default_Prediction/venv/lib/python3.8/site-packages/xgboost/core.py:246\u001b[0m, in \u001b[0;36m_check_call\u001b[0;34m(ret)\u001b[0m\n",
      "\u001b[1;32m    235\u001b[0m \u001b[39m\"\"\"Check the return value of C API call\u001b[39;00m\n",
      "\u001b[1;32m    236\u001b[0m \n",
      "\u001b[1;32m    237\u001b[0m \u001b[39mThis function will raise exception when error occurs.\u001b[39;00m\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m    243\u001b[0m \u001b[39m    return value from API calls\u001b[39;00m\n",
      "\u001b[1;32m    244\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n",
      "\u001b[1;32m    245\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[0;32m--> 246\u001b[0m     \u001b[39mraise\u001b[39;00m XGBoostError(py_str(_LIB\u001b[39m.\u001b[39mXGBGetLastError()))\n",
      "\n",
      "\u001b[0;31mXGBoostError\u001b[0m: [15:24:39] ../src/c_api/../data/../common/device_helpers.cuh:428: Memory allocation error on worker 0: std::bad_alloc: cudaErrorMemoryAllocation: out of memory\n",
      "- Free memory: 499646464\n",
      "- Requested memory: 913359440\n",
      "\n",
      "Stack trace:\n",
      "  [bt] (0) /home/victor/Documents/1-DataScience/1-Projetos/American_Express-Default_Prediction/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x3a5799) [0x7f060f33c799]\n",
      "  [bt] (1) /home/victor/Documents/1-DataScience/1-Projetos/American_Express-Default_Prediction/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x3a9bab) [0x7f060f340bab]\n",
      "  [bt] (2) /home/victor/Documents/1-DataScience/1-Projetos/American_Express-Default_Prediction/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x8da20) [0x7f060f024a20]\n",
      "  [bt] (3) /home/victor/Documents/1-DataScience/1-Projetos/American_Express-Default_Prediction/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x3f9337) [0x7f060f390337]\n",
      "  [bt] (4) /home/victor/Documents/1-DataScience/1-Projetos/American_Express-Default_Prediction/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x3fa333) [0x7f060f391333]\n",
      "  [bt] (5) /home/victor/Documents/1-DataScience/1-Projetos/American_Express-Default_Prediction/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x5d59de) [0x7f060f56c9de]\n",
      "  [bt] (6) /home/victor/Documents/1-DataScience/1-Projetos/American_Express-Default_Prediction/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x5d9384) [0x7f060f570384]\n",
      "  [bt] (7) /home/victor/Documents/1-DataScience/1-Projetos/American_Express-Default_Prediction/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x5daff5) [0x7f060f571ff5]\n",
      "  [bt] (8) /home/victor/Documents/1-DataScience/1-Projetos/American_Express-Default_Prediction/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1e5aa5) [0x7f060f17caa5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mlflow.autolog()\n",
    "importances = []\n",
    "oof = []\n",
    "TRAIN_SUBSAMPLE = 1.0\n",
    "\n",
    "skf = KFold(n_splits = 5, shuffle=True, random_state=42)\n",
    "\n",
    "for fold, (train_idx, valid_idx) in enumerate(skf.split(train, train.target)):\n",
    "\n",
    "    if TRAIN_SUBSAMPLE<1.0:\n",
    "        np.random.seed(42)\n",
    "        train_idx = np.random.choice(train_idx, \n",
    "                       int(len(train_idx)*TRAIN_SUBSAMPLE), replace=False)\n",
    "        np.random.seed(None)\n",
    "        \n",
    "    print('#'*25)\n",
    "    print('### Fold',fold+1)\n",
    "    print('### Train size',len(train_idx),'Valid size',len(valid_idx))\n",
    "    print(f'### Training with {int(TRAIN_SUBSAMPLE*100)}% fold data...')\n",
    "    print('#'*25)\n",
    "\n",
    "    X_train = train.loc[train_idx, FEATURES]\n",
    "    y_train = train.loc[train_idx, 'target']\n",
    "    X_valid = train.loc[valid_idx, FEATURES]\n",
    "    y_valid = train.loc[valid_idx, 'target']\n",
    "\n",
    "    dtrain=xgb.DMatrix(X_train, \n",
    "                        y_train)\n",
    "    del X_train, y_train\n",
    "    gc.collect()\n",
    "    d_valid = xgb.DMatrix(X_valid, \n",
    "                        y_valid)\n",
    "    del X_valid\n",
    "    gc.collect()\n",
    "    model = xgb.train(\n",
    "                    xgb_params,\n",
    "                    dtrain=dtrain,\n",
    "                    evals=[(dtrain, 'train'), (d_valid, 'test')],\n",
    "                    num_boost_round= 9999,\n",
    "                    early_stopping_rounds = 100,\n",
    "                    verbose_eval= 100\n",
    "                                                \n",
    "                    )\n",
    "\n",
    "    model.save_model(f'../models/XGB_V_fold{fold}.xgb')\n",
    "    mlflow.xgboost.log_model(model, \"XGBClassifier\")\n",
    "\n",
    "    dd = model.get_score(importance_type='weight')\n",
    "    df= pd.DataFrame({'feature':dd.keys(), f'importance_{fold}':dd.values()})\n",
    "    importances.append(df)\n",
    "    \n",
    "    oof_preds = model.predict(d_valid)\n",
    "    acc = amex_metric(y_valid.values, oof_preds)\n",
    "    mlflow.log_metric(\"Kaggle Metric for XGBClassifier\", acc)\n",
    "\n",
    "    print(\"Kaggle Metric=\", acc,'\\n')\n",
    "\n",
    "    df = train.loc[valid_idx, ['customer_ID', 'target']].copy()\n",
    "    df['oof_pred']= oof_preds\n",
    "    oof.append(df)\n",
    "\n",
    "    del   dd, df\n",
    "    del  d_valid, model\n",
    "    gc.collect()\n",
    "print('#'*25)\n",
    "oof = pd.concat(oof, axis=0, ignore_index=True).set_index('customer_ID')\n",
    "acc= amex_metric(oof.target.values, oof.oof_pred.values)\n",
    "print('OVERAL CV Kaggle Metric = ', acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reg Log && Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "reg_log_params={\n",
    "        'penalty': 'l2',\n",
    "        'max_iter': 200,\n",
    "        'warm_start': True,\n",
    "        'n_jobs': 1,\n",
    "            \n",
    "                }\n",
    "fores_params={\n",
    "        'bootstrap': True,\n",
    "        'criterion': 'gini',\n",
    "        'max_depth': 20,\n",
    "        'max_features': 'auto',\n",
    "        'max_leaf_nodes': None,\n",
    "        'min_impurity_decrease': 0.01,\n",
    "        'min_samples_leaf': 1,\n",
    "        'min_samples_split': 2,\n",
    "        'min_weight_fraction_leaf': 0.0,\n",
    "        'n_estimators': 10,\n",
    "        'n_jobs': -1,\n",
    "        'oob_score': False,\n",
    "        'random_state': 42,\n",
    "        'verbose': 0,\n",
    "        'warm_start': False\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting MLFlow\n",
    "experiment_name = \"RegLog, Forest raw dataset\"\n",
    "try:\n",
    "    exp_id = mlflow.create_experiment(name=experiment_name)\n",
    "except Exception as e:\n",
    "    exp_id = mlflow.get_experiment_by_name(experiment_name).experiment_id "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mlflow.autolog()\n",
    "\n",
    "modelclasses = [\n",
    "        #[\"reg_log\",LogisticRegression,reg_log_params],\n",
    "        [\"forest\", RandomForestClassifier, fores_params]\n",
    "                ]\n",
    "\n",
    "# TRAIN RANDOM SEED\n",
    "SEED = 42\n",
    "\n",
    "# FILL NAN VALUE\n",
    "NAN_VALUE = -127 # will fit in int8\n",
    "\n",
    "# FOLDS PER MODEL\n",
    "FOLDS = 5\n",
    "\n",
    "\n",
    "importances = []\n",
    "oof = []\n",
    "TRAIN_SUBSAMPLE = 1.0\n",
    "\n",
    "skf = KFold(n_splits = FOLDS, shuffle=True, random_state=42)\n",
    "with mlflow.start_run(experiment_id=exp_id):\n",
    "        for fold, (train_idx, valid_idx) in enumerate(skf.split(train, train.target)):\n",
    "                \n",
    "\n",
    "                if TRAIN_SUBSAMPLE<1.0:\n",
    "                        np.random.seed(SEED)\n",
    "                        train_idx = np.random.choice(train_idx, \n",
    "                                        int(len(train_idx)*TRAIN_SUBSAMPLE), replace=False)\n",
    "                        np.random.seed(None)\n",
    "                \n",
    "                X_train = train.loc[train_idx, FEATURES]\n",
    "                y_train = train.loc[train_idx, 'target']\n",
    "                X_valid = train.loc[valid_idx, FEATURES]\n",
    "                y_valid = train.loc[valid_idx, 'target']\n",
    "\n",
    "                for modelname, Model, param_list in modelclasses:\n",
    "                        print('#'*25)\n",
    "                        print('### Fold',fold+1)\n",
    "                        print('### Train size',len(train_idx),'Valid size',len(valid_idx))\n",
    "                        print(f'### Training model {modelname.upper()}')\n",
    "                        print(f'### Training with {int(TRAIN_SUBSAMPLE*100)}% fold data...')\n",
    "                        print('#'*25)\n",
    "\n",
    "                        model = Model(**param_list)\n",
    "                        #print(model)\n",
    "                        model.fit(X_train,y_train)\n",
    "\n",
    "                        pickle.dump(model, open(f'../models/{modelname}_fold{fold}.pkl','wb'))\n",
    "\n",
    "                        oof_preds = model.predict(X_valid)\n",
    "                        acc = amex_metric(y_valid.values, oof_preds)\n",
    "                        print(\"Kaggle Metric=\", acc,'\\n')\n",
    "                        mlflow.log_metric(f\"Kaggle Metric for {modelname}\", acc)\n",
    "                        mlflow.sklearn.log_model(model, f\"{Model}\")\n",
    "                        print(\"Model saved in run %s\" % mlflow.active_run().info.run_uuid)\n",
    "                        \n",
    "                        mlflow.sklearn.log_model(model, f\"{Model}\")\n",
    "                        df = train.loc[valid_idx, ['customer_ID', 'target']].copy()\n",
    "                        df['oof_pred']= oof_preds\n",
    "                        df['model_name'] = modelname\n",
    "                        oof.append(df)\n",
    "\n",
    "                        del df, model\n",
    "\n",
    "                del X_train, y_train\n",
    "                del X_valid, y_valid\n",
    "\n",
    "        \n",
    "print('#'*25)\n",
    "oof = pd.concat(oof, axis=0, ignore_index=True).set_index('customer_ID')\n",
    "for n in range(len(modelclasses)):\n",
    "    target = oof.loc[oof['model_name'] ==modelclasses[n][0], ['target'] ].reset_index()\n",
    "    preds = oof.loc[oof['model_name'] == modelclasses[n][0], ['oof_pred']].reset_index()\n",
    "    acc= amex_metric(target.target.values, preds.oof_pred.values)\n",
    "    print(f'OVERAL CV Kaggle Metric for {modelclasses[n][0]} = {acc}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting MLFlow\n",
    "experiment_name = \"LightGBM\"\n",
    "try:\n",
    "    exp_id = mlflow.create_experiment(name=experiment_name)\n",
    "except Exception as e:\n",
    "    exp_id = mlflow.get_experiment_by_name(experiment_name).experiment_id "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def lgb_amex_metric(y_pred, y_true):\n",
    "    y_true = y_true.get_label()\n",
    "    return 'amex_metric', amex_metric(y_true, y_pred), True\n",
    "\n",
    "\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'boosting': 'dart',\n",
    "    'seed': 42,\n",
    "    'num_leaves': 100,\n",
    "    'learning_rate': 0.01,\n",
    "    'feature_fraction': 0.20,\n",
    "    'bagging_freq': 10,\n",
    "    'bagging_fraction': 0.50,\n",
    "    'n_jobs': -1,\n",
    "    'lambda_l2': 2,\n",
    "    'min_data_in_leaf': 40,\n",
    "    'device_type': 'gpu',\n",
    "    'max_bin': 64,\n",
    "\n",
    "    }\n",
    "# Create a numpy array to store test predictions\n",
    "#test_predictions = np.zeros(len(test))\n",
    "# Create a numpy array to store out of folds predictions\n",
    "oof_predictions = np.zeros(len(train))\n",
    "skf = KFold(n_splits = 5, shuffle=True, random_state=42)\n",
    "for fold, (train_idx, valid_idx) in enumerate(skf.split(train, train.target)):\n",
    "    print(' ')\n",
    "    print('-'*50)\n",
    "    print(f'Training fold {fold} with {len(FEATURES)} features...')\n",
    "    x_train, x_val = train[FEATURES].iloc[train_idx], train[FEATURES].iloc[valid_idx]\n",
    "    y_train, y_val = train['target'].iloc[train_idx], train['target'].iloc[valid_idx]\n",
    "    lgb_train = lgb.Dataset(x_train, y_train)\n",
    "    lgb_valid = lgb.Dataset(x_val, y_val)\n",
    "    model = lgb.train(\n",
    "        params = params,\n",
    "        train_set = lgb_train,\n",
    "        num_boost_round = 10500,\n",
    "        valid_sets = [lgb_train, lgb_valid],\n",
    "        early_stopping_rounds = 1500,\n",
    "        verbose_eval = 500,\n",
    "        feval = lgb_amex_metric\n",
    "        )\n",
    "    # Save best model\n",
    "    pickle.dump(model, open(f'../models/LGBM_fold{fold}.pkl','wb'))\n",
    "    # Predict validation\n",
    "    val_pred = model.predict(x_val)\n",
    "    # Add to out of folds array\n",
    "    oof_predictions[valid_idx] = val_pred\n",
    "    # Predict the test set\n",
    "    #test_pred = model.predict(test[FEATURES])\n",
    "    #test_predictions += test_pred / 5\n",
    "    # Compute fold metric\n",
    "    score = amex_metric(y_val, val_pred)\n",
    "    print(f'Our fold {fold} CV score is {score}')\n",
    "    mlflow.log_metric(\"Kaggle Metric for LightGbm\", acc)\n",
    "    mlflow.lightgbm.log_model(model, f\"{Model}\")\n",
    "    del x_train, x_val, y_train, y_val, lgb_train, lgb_valid\n",
    "    gc.collect()\n",
    "# Compute out of folds metric\n",
    "score = amex_metric(train[target], oof_predictions)\n",
    "print(f'Our out of folds CV score is {score}')\n",
    "# Create a dataframe to store out of folds predictions\n",
    "oof_df = pd.DataFrame({'customer_ID': train['customer_ID'], 'target': train[target], 'prediction': oof_predictions})\n",
    "oof_df.to_csv(f'/content/drive/MyDrive/Amex/OOF/oof_lgbm_dart_baseline_5fold_seed42.csv', index = False)\n",
    "# Create a dataframe to store test prediction\n",
    "# test_df = pd.DataFrame({'customer_ID': test['customer_ID'], 'prediction': test_predictions})\n",
    "# test_df.to_csv(f'/content/drive/MyDrive/Amex/Predictions/test_lgbm_dart_baseline_fold_5_seed42.csv', index = False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "3fddb47bc9764f917e582b0801b6a9ae07a3476d8133832258babead972bf3eb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
